"""
ä¸­æ–‡æ•æ„Ÿä¿¡æ¯è¯†åˆ«å®Œæ•´ç¤ºä¾‹ï¼ˆå«LLMéªŒè¯ï¼‰

ä½¿ç”¨æ–¹å¼:
1. APIæ¨¡å¼ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰:
   python chinese_analyzer_with_llm.py --mode api --api-key sk-xxx

2. æœ¬åœ°æ¨¡å‹æ¨¡å¼ï¼ˆéœ€è¦æ¨¡å‹è·¯å¾„ï¼‰:
   python chinese_analyzer_with_llm.py --mode local --model-path /path/to/model

3. æµ‹è¯•æ¨¡å¼ï¼ˆæ— éœ€å¤–éƒ¨ä¾èµ–ï¼‰:
   python chinese_analyzer_with_llm.py --mode mock
"""

import argparse
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from presidio_analyzer import AnalyzerEngine, RecognizerRegistry
from presidio_analyzer.nlp_engine import (
    SpacyNlpEngine, 
    NerModelConfiguration,
    create_verifier,
)
from presidio_analyzer.predefined_recognizers.country_specific.china import (
    CnIdCardRecognizer,
    CnPhoneRecognizer,
    CnBankCardRecognizer,
    CnEmailRecognizer,
    CnIpAddressRecognizer,
    CnPostalCodeRecognizer,
    CnVehiclePlateRecognizer,
    CnPassportRecognizer,
    CnJwtRecognizer,
    CnCloudKeyRecognizer,
    CnNlpRecognizer,
)


def create_chinese_analyzer(use_nlp: bool = True):
    """
    åˆ›å»ºä¸­æ–‡æ•æ„Ÿä¿¡æ¯åˆ†æå¼•æ“ã€‚
    
    :param use_nlp: æ˜¯å¦ä½¿ç”¨NLPè¯†åˆ«å™¨ï¼ˆéœ€è¦spaCyä¸­æ–‡æ¨¡å‹ï¼‰
    :return: AnalyzerEngineå®ä¾‹
    """
    # åˆ›å»ºè¯†åˆ«å™¨æ³¨å†Œè¡¨
    registry = RecognizerRegistry()
    registry.supported_languages = ["zh"]
    
    # æ·»åŠ æ­£åˆ™è¯†åˆ«å™¨
    regex_recognizers = [
        CnIdCardRecognizer(),
        CnPhoneRecognizer(),
        CnBankCardRecognizer(),
        CnEmailRecognizer(),
        CnIpAddressRecognizer(),
        CnPostalCodeRecognizer(),
        CnVehiclePlateRecognizer(),
        CnPassportRecognizer(),
        CnJwtRecognizer(),
        CnCloudKeyRecognizer(),
    ]
    
    for recognizer in regex_recognizers:
        registry.add_recognizer(recognizer)
    
    nlp_engine = None
    
    if use_nlp:
        try:
            # é…ç½®NERæ¨¡å‹
            ner_config = NerModelConfiguration(
                model_to_presidio_entity_mapping={
                    "PER": "PERSON",
                    "LOC": "LOCATION",
                    "GPE": "LOCATION",
                    "ORG": "ORGANIZATION",
                },
                default_score=0.4,
            )
            
            # åˆ›å»ºNLPå¼•æ“
            nlp_engine = SpacyNlpEngine(
                models=[{"lang_code": "zh", "model_name": "zh_core_web_md"}],
                ner_model_configuration=ner_config,
            )
            nlp_engine.load()
            
            # æ·»åŠ NLPè¯†åˆ«å™¨
            registry.add_recognizer(CnNlpRecognizer())
            print("âœ… NLPå¼•æ“åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ NLPå¼•æ“åŠ è½½å¤±è´¥: {e}")
            print("   å°†åªä½¿ç”¨æ­£åˆ™è¯†åˆ«å™¨")
            use_nlp = False
    
    # åˆ›å»ºåˆ†æå¼•æ“
    analyzer = AnalyzerEngine(
        registry=registry,
        nlp_engine=nlp_engine,
        supported_languages=["zh"]
    )
    
    return analyzer


def analyze_with_llm_verification(
    text: str,
    analyzer: AnalyzerEngine,
    verifier,
    entities: list = None,
):
    """
    åˆ†ææ–‡æœ¬å¹¶è¿›è¡ŒLLMéªŒè¯ã€‚
    
    :param text: å¾…åˆ†ææ–‡æœ¬
    :param analyzer: åˆ†æå¼•æ“
    :param verifier: LLMéªŒè¯å™¨
    :param entities: è¦è¯†åˆ«çš„å®ä½“ç±»å‹åˆ—è¡¨
    :return: æœ€ç»ˆç»“æœåˆ—è¡¨
    """
    # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨è¯†åˆ«å™¨åˆ†æ
    results = analyzer.analyze(text=text, language="zh", entities=entities)
    
    if not results:
        return []
    
    # ç¬¬äºŒæ­¥ï¼šLLMéªŒè¯ä½åˆ†ç»“æœ
    verified = verifier.verify_results(text, results)
    
    # ç¬¬ä¸‰æ­¥ï¼šæ•´ç†æœ€ç»ˆç»“æœ
    final_results = []
    for original, verification in verified:
        entity_text = text[original.start:original.end]
        
        if verification:
            # ç»è¿‡LLMéªŒè¯
            if verification.is_sensitive:
                final_results.append({
                    "entity_type": original.entity_type,
                    "text": entity_text,
                    "start": original.start,
                    "end": original.end,
                    "original_score": original.score,
                    "final_score": verification.final_score,
                    "verified": True,
                    "llm_reason": verification.reason,
                })
            # å¦‚æœLLMåˆ¤æ–­ä¸æ˜¯æ•æ„Ÿä¿¡æ¯ï¼Œåˆ™ä¸åŠ å…¥ç»“æœ
        else:
            # æ— éœ€éªŒè¯ï¼Œç›´æ¥ç¡®è®¤
            final_results.append({
                "entity_type": original.entity_type,
                "text": entity_text,
                "start": original.start,
                "end": original.end,
                "original_score": original.score,
                "final_score": original.score,
                "verified": False,
                "llm_reason": None,
            })
    
    return final_results


def main():
    parser = argparse.ArgumentParser(description="ä¸­æ–‡æ•æ„Ÿä¿¡æ¯è¯†åˆ«ï¼ˆå«LLMéªŒè¯ï¼‰")
    parser.add_argument("--mode", choices=["api", "local", "mock"], default="mock",
                        help="LLMéªŒè¯æ¨¡å¼: api/local/mock")
    parser.add_argument("--api-key", help="APIå¯†é’¥ï¼ˆapiæ¨¡å¼éœ€è¦ï¼‰")
    parser.add_argument("--api-base", default="https://api.openai.com/v1",
                        help="APIåŸºç¡€URL")
    parser.add_argument("--model", default="gpt-3.5-turbo",
                        help="æ¨¡å‹åç§°ï¼ˆapiæ¨¡å¼ï¼‰æˆ–æ¨¡å‹è·¯å¾„ï¼ˆlocalæ¨¡å¼ï¼‰")
    parser.add_argument("--no-nlp", action="store_true",
                        help="ä¸ä½¿ç”¨NLPè¯†åˆ«å™¨")
    parser.add_argument("--text", help="è¦åˆ†æçš„æ–‡æœ¬")
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("ä¸­æ–‡æ•æ„Ÿä¿¡æ¯è¯†åˆ«ç³»ç»Ÿ")
    print("=" * 70)
    
    # åˆ›å»ºåˆ†æå¼•æ“
    analyzer = create_chinese_analyzer(use_nlp=not args.no_nlp)
    
    # åˆ›å»ºLLMéªŒè¯å™¨
    if args.mode == "api":
        if not args.api_key:
            print("âŒ APIæ¨¡å¼éœ€è¦æä¾› --api-key")
            sys.exit(1)
        verifier = create_verifier(
            mode="api",
            api_key=args.api_key,
            api_base=args.api_base,
            model=args.model,
        )
        print(f"âœ… LLMéªŒè¯å™¨: APIæ¨¡å¼ ({args.model})")
    elif args.mode == "local":
        verifier = create_verifier(
            mode="local",
            model_path=args.model,
        )
        print(f"âœ… LLMéªŒè¯å™¨: æœ¬åœ°æ¨¡å‹ ({args.model})")
    else:
        verifier = create_verifier(mode="mock")
        print("âœ… LLMéªŒè¯å™¨: æµ‹è¯•æ¨¡å¼")
    
    print()
    
    # æµ‹è¯•æ–‡æœ¬
    if args.text:
        test_texts = [args.text]
    else:
        test_texts = [
            "ç”¨æˆ·å¼ ä¸‰ï¼Œèº«ä»½è¯å·110101199003074518ï¼Œæ‰‹æœº13812345678ã€‚",
            "æ”¶è´§åœ°å€ï¼šåŒ—äº¬å¸‚æœé˜³åŒºä¸­å…³æ‘å¤§è¡—1å·ï¼Œé‚®ç¼–100000ã€‚",
            "å·¥ä½œå•ä½ï¼šé˜¿é‡Œå·´å·´é›†å›¢ï¼Œé‚®ç®±zhangsan@qq.comã€‚",
            "æœåŠ¡å™¨IPï¼š192.168.1.100ï¼Œç‰ˆæœ¬å·1.2.3.4ã€‚",
        ]
    
    for text in test_texts:
        print("-" * 70)
        print(f"æ–‡æœ¬: {text}")
        print()
        
        results = analyze_with_llm_verification(text, analyzer, verifier)
        
        if results:
            print("è¯†åˆ«ç»“æœ:")
            for r in results:
                verified_mark = "ğŸ”" if r["verified"] else "âœ…"
                print(f"  {verified_mark} {r['entity_type']}: \"{r['text']}\"")
                print(f"     åˆ†æ•°: {r['original_score']:.2f} â†’ {r['final_score']:.2f}")
                if r["llm_reason"]:
                    print(f"     LLMç†ç”±: {r['llm_reason']}")
        else:
            print("  æœªè¯†åˆ«åˆ°æ•æ„Ÿä¿¡æ¯")
        print()


if __name__ == "__main__":
    main()
